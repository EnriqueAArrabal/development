{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abc1283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# üîπ Par√°metros globales para comparaci√≥n justa entre modelos\n",
    "PARAMS = {\n",
    "    # Conjunto de im√°genes\n",
    "    \"max_images\": 50,          # N√∫mero m√°ximo de im√°genes a procesar\n",
    "    # Umbrales y filtros\n",
    "    \"score_thr\": 0.2,          # Umbral de score m√≠nimo para considerar detecciones\n",
    "    \"nms_iou\": 0.5,            # IoU para supresi√≥n de solapamiento (NMS)\n",
    "    \"min_box_side\": 12,        # Tama√±o m√≠nimo de lado de caja para filtrar detecciones\n",
    "    \"topk_label\": 40,          # Top-K detecciones por clase\n",
    "    # Visualizaci√≥n\n",
    "    \"score_min_draw\": 0.2,     # Umbral m√≠nimo para mostrar en plots\n",
    "    \"max_draw\": 25,          # M√°ximo de cajas a dibujar por imagen\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee788a42",
   "metadata": {},
   "source": [
    "### DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "636119d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "loading annotations into memory...\n",
      "Done (t=0.48s)\n",
      "creating index...\n",
      "index created!\n",
      "Im√°genes filtradas: 870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n",
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50] 000000000724.jpg ‚Üí 1 dets en 0.45s\n",
      "[2/50] 000000001532.jpg ‚Üí 5 dets en 0.46s\n",
      "[3/50] 000000001584.jpg ‚Üí 6 dets en 0.39s\n",
      "[4/50] 000000002006.jpg ‚Üí 1 dets en 0.48s\n",
      "[5/50] 000000005001.jpg ‚Üí 2 dets en 0.47s\n",
      "[6/50] 000000005037.jpg ‚Üí 3 dets en 0.53s\n",
      "[7/50] 000000006040.jpg ‚Üí 3 dets en 0.52s\n",
      "[8/50] 000000006723.jpg ‚Üí 6 dets en 0.50s\n",
      "[9/50] 000000007088.jpg ‚Üí 1 dets en 0.46s\n",
      "[10/50] 000000007386.jpg ‚Üí 4 dets en 0.51s\n",
      "[11/50] 000000007816.jpg ‚Üí 1 dets en 0.52s\n",
      "[12/50] 000000008211.jpg ‚Üí 3 dets en 0.50s\n",
      "[13/50] 000000008762.jpg ‚Üí 1 dets en 0.52s\n",
      "[14/50] 000000008899.jpg ‚Üí 3 dets en 0.42s\n",
      "[15/50] 000000009769.jpg ‚Üí 1 dets en 0.46s\n",
      "[16/50] 000000009891.jpg ‚Üí 2 dets en 0.48s\n",
      "[17/50] 000000010363.jpg ‚Üí 6 dets en 0.50s\n",
      "[18/50] 000000011149.jpg ‚Üí 3 dets en 0.47s\n",
      "[19/50] 000000011197.jpg ‚Üí 6 dets en 0.52s\n",
      "[20/50] 000000011511.jpg ‚Üí 2 dets en 0.50s\n",
      "[21/50] 000000011615.jpg ‚Üí 4 dets en 0.46s\n",
      "[22/50] 000000013177.jpg ‚Üí 4 dets en 0.49s\n",
      "[23/50] 000000013348.jpg ‚Üí 2 dets en 0.50s\n",
      "[24/50] 000000014380.jpg ‚Üí 3 dets en 0.49s\n",
      "[25/50] 000000015338.jpg ‚Üí 5 dets en 0.50s\n",
      "[26/50] 000000015440.jpg ‚Üí 3 dets en 0.52s\n",
      "[27/50] 000000015517.jpg ‚Üí 5 dets en 0.44s\n",
      "[28/50] 000000015597.jpg ‚Üí 3 dets en 0.49s\n",
      "[29/50] 000000017029.jpg ‚Üí 2 dets en 0.36s\n",
      "[30/50] 000000017178.jpg ‚Üí 6 dets en 0.49s\n",
      "[31/50] 000000017207.jpg ‚Üí 7 dets en 0.44s\n",
      "[32/50] 000000017627.jpg ‚Üí 8 dets en 0.45s\n",
      "[33/50] 000000018737.jpg ‚Üí 1 dets en 0.49s\n",
      "[34/50] 000000018837.jpg ‚Üí 3 dets en 0.45s\n",
      "[35/50] 000000019042.jpg ‚Üí 1 dets en 0.53s\n",
      "[36/50] 000000019109.jpg ‚Üí 11 dets en 0.49s\n",
      "[37/50] 000000021839.jpg ‚Üí 1 dets en 0.45s\n",
      "[38/50] 000000022755.jpg ‚Üí 3 dets en 0.44s\n",
      "[39/50] 000000023272.jpg ‚Üí 2 dets en 0.46s\n",
      "[40/50] 000000025393.jpg ‚Üí 2 dets en 0.45s\n",
      "[41/50] 000000026204.jpg ‚Üí 6 dets en 0.49s\n",
      "[42/50] 000000026926.jpg ‚Üí 4 dets en 0.50s\n",
      "[43/50] 000000027768.jpg ‚Üí 4 dets en 0.36s\n",
      "[44/50] 000000030828.jpg ‚Üí 3 dets en 0.51s\n",
      "[45/50] 000000031118.jpg ‚Üí 3 dets en 0.52s\n",
      "[46/50] 000000031817.jpg ‚Üí 3 dets en 0.47s\n",
      "[47/50] 000000032334.jpg ‚Üí 0 dets en 0.45s\n",
      "[48/50] 000000032941.jpg ‚Üí 6 dets en 0.47s\n",
      "[49/50] 000000033109.jpg ‚Üí 6 dets en 0.46s\n",
      "[50/50] 000000033221.jpg ‚Üí 4 dets en 0.51s\n",
      "‚úÖ Resultados guardados en grounding_dino_detections.json\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.05s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.08s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.004\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.005\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n"
     ]
    }
   ],
   "source": [
    "# üîπ Instalar dependencias (si es necesario)\n",
    "!pip install -q transformers torch torchvision pycocotools matplotlib\n",
    "\n",
    "# üîπ Imports y configuraci√≥n\n",
    "import os, json, time, gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from torchvision.ops import nms\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ==========================================\n",
    "# üîπ COCO: cargar anotaciones y filtrar im√°genes\n",
    "# ==========================================\n",
    "image_dir = \"./COCO/images/val2017\"\n",
    "annotation_file = \"./COCO/annotations/instances_val2017.json\"\n",
    "classes = [\"car\", \"bus\", \"truck\", \"motorcycle\", \"bicycle\"]\n",
    "\n",
    "coco_gt = COCO(annotation_file)\n",
    "coco_categories = coco_gt.loadCats(coco_gt.getCatIds())\n",
    "name_to_id = {cat[\"name\"]: cat[\"id\"] for cat in coco_categories}\n",
    "id_to_name = {cat[\"id\"]: cat[\"name\"] for cat in coco_categories}\n",
    "selected_cat_ids = [name_to_id[c] for c in classes if c in name_to_id]\n",
    "\n",
    "ann_ids = coco_gt.getAnnIds(catIds=selected_cat_ids)\n",
    "anns = coco_gt.loadAnns(ann_ids)\n",
    "filtered_image_ids = sorted(list({a[\"image_id\"] for a in anns}))\n",
    "filtered_images_info = coco_gt.loadImgs(filtered_image_ids)\n",
    "filtered_image_paths = [os.path.join(image_dir, img[\"file_name\"]) for img in filtered_images_info]\n",
    "print(f\"Im√°genes filtradas: {len(filtered_image_paths)}\")\n",
    "\n",
    "# ==========================================\n",
    "# üîπ Cargar modelo Grounding DINO\n",
    "# ==========================================\n",
    "model_id = \"IDEA-Research/grounding-dino-base\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device).eval()\n",
    "text_prompt = \" . \".join([f\"a {c}\" for c in classes]) + \" .\"\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# üîπ Funciones auxiliares\n",
    "# ==========================================\n",
    "def postprocess_outputs(outputs, orig_size_hw):\n",
    "    target_sizes = torch.tensor([orig_size_hw], device=outputs.logits.device)\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs=outputs,\n",
    "        target_sizes=target_sizes\n",
    "    )[0]\n",
    "    return results\n",
    "\n",
    "def filter_by_threshold(results, box_threshold=None):\n",
    "    if box_threshold is None:\n",
    "        box_threshold = PARAMS[\"score_thr\"]\n",
    "    boxes = results[\"boxes\"]\n",
    "    scores = results[\"scores\"]\n",
    "    labels = results[\"labels\"]\n",
    "    keep = [i for i, s in enumerate(scores) if s >= box_threshold]\n",
    "    return {\n",
    "        \"boxes\": boxes[keep],\n",
    "        \"scores\": scores[keep],\n",
    "        \"labels\": [labels[i] for i in keep]\n",
    "    }\n",
    "\n",
    "def apply_nms(results, iou_thr=None):\n",
    "    if iou_thr is None:\n",
    "        iou_thr = PARAMS[\"nms_iou\"]\n",
    "    boxes = results[\"boxes\"]\n",
    "    scores = results[\"scores\"]\n",
    "    labels = results[\"labels\"]\n",
    "    if len(boxes) == 0:\n",
    "        return results\n",
    "    keep_indices = []\n",
    "    unique_labels = sorted(set(labels))\n",
    "    for lab in unique_labels:\n",
    "        idxs = [i for i, l in enumerate(labels) if l == lab]\n",
    "        b = boxes[idxs]\n",
    "        s = scores[idxs]\n",
    "        keep = nms(b, s, iou_thr).cpu().numpy().tolist()\n",
    "        keep_indices.extend([idxs[i] for i in keep])\n",
    "    keep_indices = sorted(set(keep_indices))\n",
    "    return {\n",
    "        \"boxes\": boxes[keep_indices],\n",
    "        \"scores\": scores[keep_indices],\n",
    "        \"labels\": [labels[i] for i in keep_indices]\n",
    "    }\n",
    "\n",
    "def run_inference(image_paths, max_images=None, box_thr=None, nms_iou=None):\n",
    "    if max_images is None:\n",
    "        max_images = PARAMS[\"max_images\"]\n",
    "    results_all = []\n",
    "    for i, path in enumerate(image_paths[:max_images]):\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        W, H = image.size\n",
    "        with torch.no_grad():\n",
    "            inputs = processor(images=image, text=text_prompt, return_tensors=\"pt\").to(device)\n",
    "            t0 = time.time()\n",
    "            outputs = model(**inputs)\n",
    "            t1 = time.time()\n",
    "        res_raw = postprocess_outputs(outputs, (H, W))\n",
    "        res_filtered = filter_by_threshold(res_raw, box_threshold=box_thr)\n",
    "        res_nms = apply_nms(res_filtered, iou_thr=nms_iou)\n",
    "        results_all.append({\n",
    "            \"image\": path,\n",
    "            \"boxes_xyxy\": res_nms[\"boxes\"].cpu().numpy().tolist(),\n",
    "            \"labels\": res_nms[\"labels\"],\n",
    "            \"scores\": res_nms[\"scores\"].cpu().numpy().tolist(),\n",
    "            \"time\": t1 - t0\n",
    "        })\n",
    "        print(f\"[{i+1}/{min(max_images, len(image_paths))}] {os.path.basename(path)} ‚Üí {len(res_nms['labels'])} dets en {t1 - t0:.2f}s\")\n",
    "        del inputs, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    return results_all\n",
    "\n",
    "def show_result(result, score_min=None):\n",
    "    if score_min is None:\n",
    "        score_min = PARAMS[\"score_min_draw\"]\n",
    "    image = Image.open(result[\"image\"]).convert(\"RGB\")\n",
    "    fig, ax = plt.subplots(1, figsize=(9, 9))\n",
    "    ax.imshow(image)\n",
    "    for box, label, score in zip(result[\"boxes_xyxy\"], result[\"labels\"], result[\"scores\"]):\n",
    "        if score < score_min:\n",
    "            continue\n",
    "        x1, y1, x2, y2 = box\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        rect = patches.Rectangle((x1, y1), w, h, linewidth=2, edgecolor='deepskyblue', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, max(0, y1 - 5), f\"{label} ({score:.2f})\", color='black', fontsize=10, backgroundcolor='white')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# üîπ Ejecutar inferencia\n",
    "# ==========================================\n",
    "detections = run_inference(filtered_image_paths)\n",
    "\n",
    "# Visualizar primeros resultados\n",
    "if detections and detections[0][\"boxes_xyxy\"]:\n",
    "    for i in range(len(detections)):\n",
    "        #show_result(detections[i])\n",
    "        pass\n",
    "else:\n",
    "    print(\"No hay detecciones para visualizar.\")\n",
    "\n",
    "# Guardar resultados\n",
    "with open(\"grounding_dino_detections.json\", \"w\") as f:\n",
    "    json.dump(detections, f, indent=2)\n",
    "print(\"‚úÖ Resultados guardados en grounding_dino_detections.json\")\n",
    "\n",
    "# ==========================================\n",
    "# üîπ Evaluaci√≥n COCO\n",
    "# ==========================================\n",
    "def to_coco_detections(detections, name_to_id, score_min=None):\n",
    "    if score_min is None:\n",
    "        score_min = 0.001\n",
    "    coco_dets = []\n",
    "    for img_info, det in zip(filtered_images_info, detections):\n",
    "        img_id = img_info[\"id\"]\n",
    "        for box, label, score in zip(det[\"boxes_xyxy\"], det[\"labels\"], det[\"scores\"]):\n",
    "            if score < score_min or label not in name_to_id:\n",
    "                continue\n",
    "            x1, y1, x2, y2 = box\n",
    "            x, y, w, h = float(x1), float(y1), float(x2 - x1), float(y2 - y1)\n",
    "            coco_dets.append({\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": name_to_id[label],\n",
    "                \"bbox\": [x, y, w, h],\n",
    "                \"score\": float(score)\n",
    "            })\n",
    "    return coco_dets\n",
    "\n",
    "coco_dets = to_coco_detections(detections, name_to_id)\n",
    "if coco_dets:\n",
    "    with open(\"coco_results_groundingdino.json\", \"w\") as f:\n",
    "        json.dump(coco_dets, f)\n",
    "    coco_dt = coco_gt.loadRes(\"coco_results_groundingdino.json\")\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    coco_eval.params.imgIds = filtered_image_ids[:PARAMS[\"max_images\"]]\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No hay predicciones para evaluar.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951dc298",
   "metadata": {},
   "source": [
    "### OMDET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc29876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# üîπ OmDetTurbo: inferencia + postproceso + COCO eval\n",
    "# ===========================================\n",
    "!pip install -q transformers torch torchvision pycocotools matplotlib tqdm\n",
    "\n",
    "import os, json, gc, time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from torchvision.ops import nms\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from transformers import AutoProcessor, OmDetTurboForObjectDetection\n",
    "\n",
    "# ===============================\n",
    "# 0. Configuraci√≥n inicial\n",
    "# ===============================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# dataset COCO\n",
    "image_dir = \"./COCO/images/val2017\"\n",
    "annotation_file = \"./COCO/annotations/instances_val2017.json\"\n",
    "coco_gt = COCO(annotation_file)\n",
    "\n",
    "# clases de inter√©s\n",
    "classes = [\"car\", \"bus\", \"truck\", \"motorcycle\", \"bicycle\"]\n",
    "text_queries = [f\"a {c}\" for c in classes]\n",
    "\n",
    "# mapear nombres a IDs COCO\n",
    "cats = coco_gt.loadCats(coco_gt.getCatIds())\n",
    "name_to_id = {c[\"name\"]: c[\"id\"] for c in cats}\n",
    "selected_cat_ids = [name_to_id[c] for c in classes if c in name_to_id]\n",
    "\n",
    "# filtrar im√°genes relevantes\n",
    "ann_ids = coco_gt.getAnnIds(catIds=selected_cat_ids)\n",
    "anns = coco_gt.loadAnns(ann_ids)\n",
    "filtered_image_ids = sorted(list({a[\"image_id\"] for a in anns}))\n",
    "filtered_images_info = coco_gt.loadImgs(filtered_image_ids)\n",
    "filtered_image_paths = [os.path.join(image_dir, img[\"file_name\"]) for img in filtered_images_info]\n",
    "print(f\"Im√°genes filtradas: {len(filtered_image_paths)}\")\n",
    "\n",
    "# ===============================\n",
    "# 1. Cargar modelo OmDetTurbo\n",
    "# ===============================\n",
    "processor = AutoProcessor.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\")\n",
    "model = OmDetTurboForObjectDetection.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\").to(device).eval()\n",
    "\n",
    "def clip_boxes_xyxy(boxes, W, H):\n",
    "    boxes[:, 0] = boxes[:, 0].clamp(0, W - 1)\n",
    "    boxes[:, 1] = boxes[:, 1].clamp(0, H - 1)\n",
    "    boxes[:, 2] = boxes[:, 2].clamp(0, W - 1)\n",
    "    boxes[:, 3] = boxes[:, 3].clamp(0, H - 1)\n",
    "    return boxes\n",
    "\n",
    "def filter_small_boxes(boxes, scores, labels, min_side=None):\n",
    "    if min_side is None:\n",
    "        min_side = PARAMS[\"min_box_side\"]\n",
    "    if len(boxes) == 0:\n",
    "        return boxes, scores, labels\n",
    "    w = boxes[:, 2] - boxes[:, 0]\n",
    "    h = boxes[:, 3] - boxes[:, 1]\n",
    "    keep = (w >= min_side) & (h >= min_side)\n",
    "    return boxes[keep], scores[keep], [labels[i] for i, k in enumerate(keep.tolist()) if k]\n",
    "\n",
    "def topk_per_label(boxes, scores, labels, k=None):\n",
    "    if k is None:\n",
    "        k = PARAMS[\"topk_label\"]\n",
    "    if len(boxes) == 0:\n",
    "        return boxes, scores, labels\n",
    "    keep_idx = []\n",
    "    uniq = sorted(set(labels))\n",
    "    for lab in uniq:\n",
    "        idxs = [i for i, l in enumerate(labels) if l == lab]\n",
    "        if not idxs:\n",
    "            continue\n",
    "        idxs_sorted = sorted(idxs, key=lambda i: float(scores[i]), reverse=True)[:k]\n",
    "        keep_idx.extend(idxs_sorted)\n",
    "    keep_idx = sorted(set(keep_idx))\n",
    "    return boxes[keep_idx], scores[keep_idx], [labels[i] for i in keep_idx]\n",
    "\n",
    "def postprocess_omdet(outputs, orig_size_hw):\n",
    "    H, W = orig_size_hw\n",
    "\n",
    "    # Obtener logits y coordenadas\n",
    "    logits = outputs.decoder_class_logits[0].cpu()        # [N, C]\n",
    "    boxes_cxcywh = outputs.decoder_coord_logits[0].cpu()  # [N, 4] en cxcywh normalizado\n",
    "\n",
    "    # Calcular scores y labels\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    scores, label_idx = probs.max(dim=-1)\n",
    "\n",
    "    # Filtrar por score m√≠nimo\n",
    "    keep = scores >= PARAMS[\"score_thr\"]\n",
    "    scores = scores[keep]\n",
    "    label_idx = label_idx[keep]\n",
    "    boxes_cxcywh = boxes_cxcywh[keep]\n",
    "\n",
    "    # Convertir cxcywh ‚Üí xyxy en p√≠xeles\n",
    "    cx, cy, bw, bh = boxes_cxcywh.unbind(-1)\n",
    "    cx *= W; cy *= H; bw *= W; bh *= H\n",
    "    x1 = cx - bw / 2\n",
    "    y1 = cy - bh / 2\n",
    "    x2 = cx + bw / 2\n",
    "    y2 = cy + bh / 2\n",
    "    boxes = torch.stack([x1, y1, x2, y2], dim=-1)\n",
    "    boxes = clip_boxes_xyxy(boxes, W, H)\n",
    "\n",
    "    # Mapear √≠ndices a texto\n",
    "    labels = [text_queries[i] if 0 <= i < len(text_queries) else f\"Clase_{i}\" for i in label_idx.tolist()]\n",
    "\n",
    "    # Aplicar filtros adicionales\n",
    "    boxes, scores, labels = filter_small_boxes(boxes, scores, labels)\n",
    "    boxes, scores, labels = topk_per_label(boxes, scores, labels)\n",
    "\n",
    "    return {\"boxes\": boxes, \"scores\": scores, \"labels\": labels}\n",
    "\n",
    "\n",
    "\n",
    "def apply_nms_by_label(results, iou_thr=None):\n",
    "    if iou_thr is None:\n",
    "        iou_thr = PARAMS[\"nms_iou\"]\n",
    "    boxes, scores, labels = results[\"boxes\"], results[\"scores\"], results[\"labels\"]\n",
    "    if len(boxes) == 0:\n",
    "        return results\n",
    "    keep_all = []\n",
    "    for lab in sorted(set(labels)):\n",
    "        idxs = [i for i, l in enumerate(labels) if l == lab]\n",
    "        if not idxs:\n",
    "            continue\n",
    "        b = boxes[idxs]\n",
    "        s = scores[idxs]\n",
    "        kept = nms(b, s, iou_thr).cpu().numpy().tolist()\n",
    "        keep_all.extend([idxs[i] for i in kept])\n",
    "    keep_all = sorted(set(keep_all))\n",
    "    return {\n",
    "        \"boxes\": boxes[keep_all],\n",
    "        \"scores\": scores[keep_all],\n",
    "        \"labels\": [labels[i] for i in keep_all]\n",
    "    }\n",
    "\n",
    "# ===============================\n",
    "# 3. Inferencia\n",
    "# ===============================\n",
    "def run_omdet_inference(image_paths, max_images=None):\n",
    "    if max_images is None:\n",
    "        max_images = PARAMS[\"max_images\"]\n",
    "    results_all = []\n",
    "    for i, path in enumerate(tqdm(image_paths[:max_images], desc=\"Inferencia OmDetTurbo\")):\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        W, H = image.size\n",
    "        with torch.no_grad():\n",
    "            inputs = processor(images=image, text=text_queries, return_tensors=\"pt\").to(device)\n",
    "            t0 = time.time()\n",
    "            outputs = model(**inputs)\n",
    "            t1 = time.time()\n",
    "        res = postprocess_omdet(outputs, (H, W))\n",
    "        res_nms = apply_nms_by_label(res)\n",
    "        results_all.append({\n",
    "            \"image\": path,\n",
    "            \"boxes_xyxy\": res_nms[\"boxes\"].numpy().tolist(),\n",
    "            \"labels\": res_nms[\"labels\"],\n",
    "            \"scores\": res_nms[\"scores\"].numpy().tolist(),\n",
    "            \"time\": t1 - t0\n",
    "        })\n",
    "        del inputs, outputs\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "    return results_all\n",
    "\n",
    "# ===============================\n",
    "# 4. Visualizaci√≥n\n",
    "# ===============================\n",
    "def mostrar_10_resultados_omdet(resultados):\n",
    "    for idx, resultado in enumerate(resultados[:10]):\n",
    "        image = Image.open(resultado[\"image\"]).convert(\"RGB\")\n",
    "        fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "        ax.imshow(image)\n",
    "        drawn = 0\n",
    "        for box, label, score in sorted(zip(resultado[\"boxes_xyxy\"], resultado[\"labels\"], resultado[\"scores\"]),\n",
    "                                        key=lambda x: x[2], reverse=True):\n",
    "            if score < PARAMS[\"score_min_draw\"]:\n",
    "                continue\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                     linewidth=2, edgecolor='lime', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x1, max(0, y1 - 5),\n",
    "                    f\"{label} ({score:.2f})\", color='black',\n",
    "                    fontsize=9, backgroundcolor='white')\n",
    "            drawn += 1\n",
    "            if drawn >= PARAMS[\"max_draw\"]:\n",
    "                break\n",
    "        ax.set_title(f\"Imagen {idx+1} ‚Äî {drawn} detecciones (‚â• {PARAMS['score_min_draw']})\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "# ===============================\n",
    "# 5. Exportar detecciones a COCO\n",
    "# ===============================\n",
    "def to_coco_dets(dets, filtered_images_info, name_to_id, score_min=0.001):\n",
    "    coco_dets = []\n",
    "    for img_info, d in zip(filtered_images_info, dets):\n",
    "        img_id = img_info[\"id\"]\n",
    "        for (x1, y1, x2, y2), lab, sc in zip(d[\"boxes_xyxy\"], d[\"labels\"], d[\"scores\"]):\n",
    "            cname = lab.replace(\"a \", \"\")\n",
    "            if sc < score_min or cname not in name_to_id:\n",
    "                continue\n",
    "            coco_dets.append({\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": name_to_id[cname],\n",
    "                \"bbox\": [float(x1), float(y1), float(x2 - x1), float(y2 - y1)],\n",
    "                \"score\": float(sc)\n",
    "            })\n",
    "    return coco_dets\n",
    "\n",
    "# ===============================\n",
    "# 6. Ejecutar todo\n",
    "# ===============================\n",
    "omdet_results = run_omdet_inference(filtered_image_paths)\n",
    "#mostrar_10_resultados_omdet(omdet_results)\n",
    "\n",
    "# guardar resultados\n",
    "with open(\"omdet_results.json\", \"w\") as f:\n",
    "    json.dump(omdet_results, f, indent=2)\n",
    "print(\"‚úÖ Resultados guardados en omdet_results.json\")\n",
    "\n",
    "# evaluaci√≥n COCO\n",
    "coco_dets = to_coco_dets(omdet_results, filtered_images_info[:len(omdet_results)], name_to_id, score_min=0.001)\n",
    "if coco_dets:\n",
    "    with open(\"coco_results_omdet.json\", \"w\") as f:\n",
    "        json.dump(coco_dets, f)\n",
    "    coco_dt = coco_gt.loadRes(\"coco_results_omdet.json\")\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    coco_eval.params.imgIds = filtered_image_ids[:len(omdet_results)]\n",
    "    coco_eval.evaluate(); coco_eval.accumulate(); coco_eval.summarize()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No hay predicciones para evaluar.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d81a3d",
   "metadata": {},
   "source": [
    "### OWLv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2930bbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 196\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m coco_dets\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# üîπ Ejecutar inferencia y visualizaci√≥n\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m owlv2_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_owlv2_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_image_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m#mostrar_10_resultados_owlv2(owlv2_results)\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Guardar resultados\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mowlv2_results.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[9], line 146\u001b[0m, in \u001b[0;36mrun_owlv2_inference\u001b[1;34m(image_paths)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(PARAMS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_images\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(image_paths))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ‚Üí \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(res_nms[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dets en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt1\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt0\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m inputs, outputs\n\u001b[1;32m--> 146\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache(); \u001b[43mgc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results_all\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# üîπ OWLv2: inferencia afinada + visualizaci√≥n + evaluaci√≥n COCO\n",
    "!pip install -q transformers torch torchvision pycocotools matplotlib\n",
    "\n",
    "import os, json, time, gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from torchvision.ops import nms\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "# ===============================\n",
    "# üîπ CONFIGURACI√ìN B√ÅSICA\n",
    "# ===============================\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IMAGE_DIR = \"./COCO/images/val2017\"\n",
    "ANNOTATION_FILE = \"./COCO/annotations/instances_val2017.json\"\n",
    "CLASSES = [\"car\", \"bus\", \"truck\", \"motorcycle\", \"bicycle\"]\n",
    "TEXT_QUERIES = [f\"a {c}\" for c in CLASSES]\n",
    "MODEL_ID = \"google/owlv2-base-patch16-ensemble\"\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ===============================\n",
    "# üîπ COCO: cargar y filtrar im√°genes\n",
    "# ===============================\n",
    "coco_gt = COCO(ANNOTATION_FILE)\n",
    "cats = coco_gt.loadCats(coco_gt.getCatIds())\n",
    "name_to_id = {c[\"name\"]: c[\"id\"] for c in cats}\n",
    "selected_cat_ids = [name_to_id[c] for c in CLASSES if c in name_to_id]\n",
    "\n",
    "ann_ids = coco_gt.getAnnIds(catIds=selected_cat_ids)\n",
    "anns = coco_gt.loadAnns(ann_ids)\n",
    "filtered_image_ids = sorted(list({a[\"image_id\"] for a in anns}))\n",
    "filtered_images_info = coco_gt.loadImgs(filtered_image_ids)\n",
    "filtered_image_paths = [os.path.join(IMAGE_DIR, img[\"file_name\"]) for img in filtered_images_info]\n",
    "print(f\"Im√°genes filtradas: {len(filtered_image_paths)}\")\n",
    "\n",
    "# ===============================\n",
    "# üîπ Cargar modelo OWLv2\n",
    "# ===============================\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(MODEL_ID).to(DEVICE).eval()\n",
    "\n",
    "# ===============================\n",
    "# üîπ Funciones auxiliares\n",
    "# ===============================\n",
    "def clip_boxes_xyxy(boxes, W, H):\n",
    "    boxes[:, 0] = boxes[:, 0].clamp(0, W-1)\n",
    "    boxes[:, 1] = boxes[:, 1].clamp(0, H-1)\n",
    "    boxes[:, 2] = boxes[:, 2].clamp(0, W-1)\n",
    "    boxes[:, 3] = boxes[:, 3].clamp(0, H-1)\n",
    "    return boxes\n",
    "\n",
    "def filter_small_boxes(boxes, scores, labels, min_side=None):\n",
    "    if min_side is None:\n",
    "        min_side = PARAMS[\"min_box_side\"]\n",
    "    if len(boxes) == 0:\n",
    "        return boxes, scores, labels\n",
    "    w = boxes[:, 2] - boxes[:, 0]\n",
    "    h = boxes[:, 3] - boxes[:, 1]\n",
    "    keep = (w >= min_side) & (h >= min_side)\n",
    "    return boxes[keep], scores[keep], [labels[i] for i, k in enumerate(keep.tolist()) if k]\n",
    "\n",
    "def topk_per_label(boxes, scores, labels, k=None):\n",
    "    if k is None:\n",
    "        k = PARAMS[\"topk_label\"]\n",
    "    if len(boxes) == 0:\n",
    "        return boxes, scores, labels\n",
    "    keep_idx = []\n",
    "    for lab in sorted(set(labels)):\n",
    "        idxs = [i for i, l in enumerate(labels) if l == lab]\n",
    "        if not idxs:\n",
    "            continue\n",
    "        idxs_sorted = sorted(idxs, key=lambda i: float(scores[i]), reverse=True)[:k]\n",
    "        keep_idx.extend(idxs_sorted)\n",
    "    keep_idx = sorted(set(keep_idx))\n",
    "    return boxes[keep_idx], scores[keep_idx], [labels[i] for i in keep_idx]\n",
    "\n",
    "def postprocess_owlv2(outputs, orig_size_hw, score_thr=None, min_box_side=None, topk_label=None):\n",
    "    if score_thr is None:\n",
    "        score_thr = PARAMS[\"score_thr\"]\n",
    "    H, W = orig_size_hw\n",
    "    target_sizes = torch.tensor([[H, W]], device=outputs.logits.device)\n",
    "    results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes)[0]\n",
    "    boxes = results[\"boxes\"].cpu()\n",
    "    scores = results[\"scores\"].cpu()\n",
    "    labels_idx = results[\"labels\"].cpu().tolist()\n",
    "    labels = [TEXT_QUERIES[i] if 0 <= i < len(TEXT_QUERIES) else f\"Clase_{i}\" for i in labels_idx]\n",
    "\n",
    "    keep = scores >= score_thr\n",
    "    boxes = boxes[keep]\n",
    "    scores = scores[keep]\n",
    "    labels = [labels[i] for i, k in enumerate(keep.tolist()) if k]\n",
    "\n",
    "    boxes = clip_boxes_xyxy(boxes, W, H)\n",
    "    boxes, scores, labels = filter_small_boxes(boxes, scores, labels, min_side=min_box_side)\n",
    "    boxes, scores, labels = topk_per_label(boxes, scores, labels, k=topk_label)\n",
    "    return {\"boxes\": boxes, \"scores\": scores, \"labels\": labels}\n",
    "\n",
    "def apply_nms_by_label(results, iou_thr=None):\n",
    "    if iou_thr is None:\n",
    "        iou_thr = PARAMS[\"nms_iou\"]\n",
    "    boxes, scores, labels = results[\"boxes\"], results[\"scores\"], results[\"labels\"]\n",
    "    if len(boxes) == 0:\n",
    "        return results\n",
    "    keep_all = []\n",
    "    for lab in sorted(set(labels)):\n",
    "        idxs = [i for i, l in enumerate(labels) if l == lab]\n",
    "        if not idxs:\n",
    "            continue\n",
    "        b = boxes[idxs]\n",
    "        s = scores[idxs]\n",
    "        kept = nms(b, s, iou_thr).cpu().numpy().tolist()\n",
    "        keep_all.extend([idxs[i] for i in kept])\n",
    "    keep_all = sorted(set(keep_all))\n",
    "    return {\"boxes\": boxes[keep_all], \"scores\": scores[keep_all], \"labels\": [labels[i] for i in keep_all]}\n",
    "\n",
    "# ===============================\n",
    "# üîπ Inferencia\n",
    "# ===============================\n",
    "def run_owlv2_inference(image_paths):\n",
    "    results_all = []\n",
    "    for i, path in enumerate(image_paths[:PARAMS[\"max_images\"]]):\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        W, H = image.size\n",
    "        with torch.no_grad():\n",
    "            inputs = processor(images=image, text=TEXT_QUERIES, return_tensors=\"pt\").to(DEVICE)\n",
    "            t0 = time.time()\n",
    "            outputs = model(**inputs)\n",
    "            t1 = time.time()\n",
    "        res = postprocess_owlv2(outputs, (H, W))\n",
    "        res_nms = apply_nms_by_label(res)\n",
    "        results_all.append({\n",
    "            \"image\": path,\n",
    "            \"boxes_xyxy\": res_nms[\"boxes\"].numpy().tolist(),\n",
    "            \"labels\": res_nms[\"labels\"],\n",
    "            \"scores\": res_nms[\"scores\"].numpy().tolist(),\n",
    "            \"time\": t1 - t0\n",
    "        })\n",
    "        print(f\"[{i+1}/{min(PARAMS['max_images'], len(image_paths))}] {os.path.basename(path)} ‚Üí {len(res_nms['labels'])} dets en {t1 - t0:.2f}s\")\n",
    "        del inputs, outputs\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "    return results_all\n",
    "\n",
    "# ===============================\n",
    "# üîπ Visualizaci√≥n\n",
    "# ===============================\n",
    "def mostrar_10_resultados_owlv2(resultados):\n",
    "    for idx, resultado in enumerate(resultados[:10]):\n",
    "        image = Image.open(resultado[\"image\"]).convert(\"RGB\")\n",
    "        fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "        ax.imshow(image)\n",
    "        drawn = 0\n",
    "        for box, label, score in sorted(zip(resultado[\"boxes_xyxy\"], resultado[\"labels\"], resultado[\"scores\"]),\n",
    "                                        key=lambda x: x[2], reverse=True):\n",
    "            if score < PARAMS[\"score_min_draw\"]:\n",
    "                continue\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='lime', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x1, max(0, y1 - 5), f\"{label} ({score:.2f})\", color='black', fontsize=9, backgroundcolor='white')\n",
    "            drawn += 1\n",
    "            if drawn >= PARAMS[\"max_draw\"]:\n",
    "                break\n",
    "        ax.set_title(f\"Imagen {idx+1} ‚Äî {drawn} dets (‚â• {PARAMS['score_min_draw']})\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "# ===============================\n",
    "# üîπ COCO detections\n",
    "# ===============================\n",
    "def to_coco_dets(dets, filtered_images_info, name_to_id, score_min=0.001):\n",
    "    coco_dets = []\n",
    "    for img_info, d in zip(filtered_images_info, dets):\n",
    "        img_id = img_info[\"id\"]\n",
    "        for (x1, y1, x2, y2), lab, sc in zip(d[\"boxes_xyxy\"], d[\"labels\"], d[\"scores\"]):\n",
    "            cname = lab.replace(\"a \", \"\")\n",
    "            if sc < score_min or cname not in name_to_id:\n",
    "                continue\n",
    "            coco_dets.append({\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": name_to_id[cname],\n",
    "                \"bbox\": [float(x1), float(y1), float(x2 - x1), float(y2 - y1)],\n",
    "                \"score\": float(sc)\n",
    "            })\n",
    "    return coco_dets\n",
    "\n",
    "# ===============================\n",
    "# üîπ Ejecutar inferencia y visualizaci√≥n\n",
    "# ===============================\n",
    "owlv2_results = run_owlv2_inference(filtered_image_paths)\n",
    "#mostrar_10_resultados_owlv2(owlv2_results)\n",
    "\n",
    "# Guardar resultados\n",
    "with open(\"owlv2_results.json\", \"w\") as f:\n",
    "    json.dump(owlv2_results, f, indent=2)\n",
    "print(\"‚úÖ Resultados guardados en owlv2_results.json\")\n",
    "\n",
    "# Evaluar mAP COCO\n",
    "coco_dets = to_coco_dets(owlv2_results, filtered_images_info[:len(owlv2_results)], name_to_id)\n",
    "if coco_dets:\n",
    "    with open(\"coco_results_owlv2.json\", \"w\") as f:\n",
    "        json.dump(coco_dets, f)\n",
    "    coco_dt = coco_gt.loadRes(\"coco_results_owlv2.json\")\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    coco_eval.params.imgIds = filtered_image_ids[:len(owlv2_results)]\n",
    "    coco_eval.evaluate(); coco_eval.accumulate(); coco_eval.summarize()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No hay predicciones para evaluar.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
